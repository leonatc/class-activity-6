---
title: 'HUDK4050: Class Activity 6'
author: "Charles Lang"
date: "10/23/2018"
output: html_document
---
# Data Management
```{r}
#
library(dplyr)
library(tidyr)

#Load data
DF1 <- read.csv("HUDK405018-cluster.csv", header = TRUE)
  
#Wrangle data
DF1 <- DF1[3,] 
df3 <- DF1[-c(1,2), ]  
df4 <- select(df3, Q1_1, Q1_2)
```

# Find lattitudes & longitudes for cities
```{r}
install.packages("ggmap")
library(ggmap)

#Request lattitude and longitude from Google Maps API
#Where did you live before you began your degree at Teachers College? - City
DF2 <- geocode(as.character(DF1$Q1_1), output = "latlon", source = "dsk")
df_ll <- geocode(as.character(df4$Q1_1), output = "latlon", source = "dsk")
```

Now we will run the K-means clustering algorithm we talked about in class. 
1) The algorithm starts by randomly choosing some starting values 
2) Associates all observations near to those values with them
3) Calculates the mean of those clusters of values
4) Selects the observation closest to the mean of the cluster
5) Re-associates all observations closest to this observation
6) Continues this process until the clusters are no longer changing

Notice that in this case we have 5 variables and in class we only had 2. It is impossible to vizualise this process with 5 variables.

Also, we need to choose the number of clusters we think are in the data. We will start with 4.

```{r}
??kmeans
fit <- kmeans(DF_LL, 1) 
plot(DF_LL$lon, DF_LL$lat)
results <- kmeans(DF_LL, 3)

#We have created an object called "fit" that contains all the details of our clustering including which observations belong to each cluster.

#We can access the list of clusters by typing "fit$cluster", the top row corresponds to the original order the rows were in. Notice we have deleted some rows.

fit$cluster
results$cluster

#We can also attach these clusters to te original dataframe by using the "data.frame" command to create a new data frame called K4.

DF3 <- data.frame(DF2, fit$cluster)
df_new <- data.frame(df_ll, results$cluster)

#Have a look at the K4 dataframe. Lets change the names of the variables to make it more convenient with the names() command.

names(DF3) <- c("1", "2", "3", "4", "5", "cluster") #c() stands for concatonate and it creates a vector of anything, in this case a vector of names.

names(df_new) <- c("long", "lat", "cluster")

```

# Visualize your clusters in ggplot
```{r}
#Create a scatterplot that plots location of each student and colors the points according to their cluster 
plot(df_new[c("lat", "long")], col = df_new$cluster)

library(ggplot2)
f <- ggplot(df_new, aes(lat, long)) + geom_jitter(color = df_new$cluster)
f
```

# Can you group students according to their weekly activities?

```{r}
install.packages("factoextra")
install.packages("cluster")
install.packages("magrittr")
library("cluster")
library("factoextra")
library("magrittr")
stu <- DF1[-c(1,2), 20:26]
stu[] <- lapply(stu, as.numeric)

# for computing a distance matrix between the rows of a data matrix. 
# Compared to the standard dist() function, 
# it supports correlation-based distance measures including “pearson”, “kendall” and “spearman” methods.
res.dist <- get_dist(stu, stand = TRUE, method = "pearson")

# for visualizing a distance matrix
fviz_dist(res.dist, 
   gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))


res.dist2 <- get_dist(df_ll, stand = TRUE, method = "pearson")
fviz_dist(res.dist2, 
   gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))

```

```{r}
fviz_nbclust(stu, kmeans, method = "gap_stat")

fviz_nbclust(df_ll, kmeans, method = "gap_stat")
```
```{r}
my_data <- stu
my_data <- df_ll

set.seed(123)
km.res <- kmeans(my_data, 3, nstart = 25)
# Visualize
library("factoextra")
fviz_cluster(km.res, data = my_data,
             ellipse.type = "convex",
             palette = "jco",
             ggtheme = theme_minimal())
```

```{r}
# Compute hierarchical clustering
res.hc <- stu %>%
  scale() %>%                    # Scale the data
  dist(method = "euclidean") %>% # Compute dissimilarity matrix
  hclust(method = "ward.D2")     # Compute hierachical clustering
# Visualize using factoextra
# Cut in 4 groups and color by groups
fviz_dend(res.hc, k = 4, # Cut in four groups
          cex = 0.5, # label size
          k_colors = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
          color_labels_by_k = TRUE, # color labels by groups
          rect = TRUE # Add rectangle around groups
          )
```

```{r}
gradient.color <- list(low = "steelblue",  high = "white")
stu %>%    # Remove column 5 (Species)
  scale() %>%     # Scale variables
  get_clust_tendency(n = 40, gradient = gradient.color)
```

```{r}
set.seed(123)
# Compute
install.packages("NbClust")
library("NbClust")
```

```{r}
res.nbclust <- stu %>%
  scale() %>%
  NbClust(distance = "euclidean",
          min.nc = 2, max.nc = 10, 
          method = "complete", index ="all") 

# Visualize
library(factoextra)
fviz_nbclust(res.nbclust, ggtheme = theme_minimal())
```

